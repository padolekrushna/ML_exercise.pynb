# ========================================
# üîÅ STEP 1: Import Modules and Load Dataset
# ========================================
import numpy as np
import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import torch
import torch.nn as nn
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from torch.utils.data import Dataset
from transformers import (
    GPT2LMHeadModel, GPT2Tokenizer, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling,
    AutoTokenizer, AutoModelForSeq2SeqLM
)
import warnings
warnings.filterwarnings('ignore')

nltk.download("stopwords")
nltk.download("punkt")
nltk.download("vader_lexicon")

# ========================================
# üîÅ STEP 2: Load and Preprocess Emotion Dataset
# ========================================
Emotion_data = pd.read_csv("Emotions.csv")
Emotion_data = Emotion_data.drop(["Unnamed: 0"], axis=1)
E_data = pd.DataFrame()
for i in range(6):
    subset = Emotion_data[Emotion_data["label"] == i].sample(n=12000, random_state=42)
    E_data = pd.concat([E_data, subset])
Emotion_data = E_data.copy()
Emotion_data.reset_index(drop=True, inplace=True)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    text = re.sub(r"\W", " ", text)
    text = re.sub(r"\d+", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words("english")]
    return " ".join(tokens)

Emotion_data["cleaned_text"] = Emotion_data["text"]
Emotion_data = Emotion_data.drop(["text"], axis=1)
Emotion_data.dropna(inplace=True)
Emotion_data["word_count"] = Emotion_data["cleaned_text"].apply(lambda x: len(x.split()))
Emotion_data["char_count"] = Emotion_data["cleaned_text"].apply(lambda x: len(x))
Emotion_data.drop(["word_count", "char_count"], axis=1, inplace=True)

# ========================================
# üîÅ STEP 3: TF-IDF + Sentiment Features
# ========================================
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_tfidf = tfidf.fit_transform(Emotion_data["cleaned_text"])
sia = SentimentIntensityAnalyzer()
Emotion_data["sentiment"] = Emotion_data["cleaned_text"].apply(lambda x: sia.polarity_scores(x)["compound"])
X = np.hstack((X_tfidf.toarray(), Emotion_data["sentiment"].values.reshape(-1, 1)))
y = Emotion_data["label"]

# ========================================
# üîÅ STEP 4: Train/Test Split + Logistic Regression Model
# ========================================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
log_reg = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')
log_reg.fit(X_train, y_train)
with open('logistic_regression_model.pkl', 'wb') as f:
    pickle.dump(log_reg, f)
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

# ========================================
# üîÅ STEP 5: Define GPT-2 Fine-Tuned Model Class
# ========================================
class MentalHealthDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

def create_comprehensive_training_data():
    # Your full pre-written training data logic (unchanged)
    return [  # dummy placeholder: replace with your long conversation list
        "[SAD] I feel empty and hopeless -> 1. Start small. 2. Talk to someone. 3. Rest. 4. Reflect."
    ]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class MentalHealthLLM:
    def __init__(self, model_name='gpt2'):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = device

    def initialize_model(self):
        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
        self.tokenizer.add_special_tokens({
            'pad_token': '<|pad|>',
            'bos_token': '<|startoftext|>',
            'eos_token': '<|endoftext|>'
        })
        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
        self.model.resize_token_embeddings(len(self.tokenizer))
        self.model.to(self.device)

    def prepare_training_data(self):
        conversations = create_comprehensive_training_data()
        return [f"<|startoftext|>{conv}<|endoftext|>" for conv in conversations]

    def train_model(self, epochs=2, batch_size=1):
        training_data = self.prepare_training_data()
        dataset = MentalHealthDataset(training_data, self.tokenizer)
        training_args = TrainingArguments(
            output_dir='./mental_health_gpt2',
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            logging_dir='./logs',
            save_strategy="epoch"
        )
        data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        trainer.train()

    def generate_response(self, emotion, user_text, max_length=300):
        input_text = f"<|startoftext|>[{emotion.upper()}] {user_text} ->"
        inputs = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=inputs.shape[1] + max_length,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                pad_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=2,
                early_stopping=True
            )
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response[len(input_text):].strip().split("<|endoftext|>")[0].strip()

    def predict(self, emotion, user_text):
        try:
            return self.generate_response(emotion, user_text)
        except:
            return "Fallback response."

# ========================================
# üîÅ STEP 6: Define FLAN-T5 Lightweight LLM
# ========================================
class LightMentalHealthLLM:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = "google/flan-t5-small"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device)

    def get_advice(self, emotion, user_text):
        prompt = f"The user said: A person is feeling {emotion}. Suggest 3 to 4 helpful mental health strategies."
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=64).to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                num_beams=3
            )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# ========================================
# ‚úÖ FINAL STEP: RUNNING THE COMPLETE FLOW
# ========================================
print("\nüß† AI Mental Health Assistant")
test_text = input("üìù How are you feeling today? ")

# Emotion Detection
with open("logistic_regression_model.pkl", "rb") as f:
    model = pickle.load(f)
with open("tfidf_vectorizer.pkl", "rb") as f:
    tfidf_vectorizer = pickle.load(f)

X_input = tfidf_vectorizer.transform([test_text]).toarray()
sentiment = np.array([[sia.polarity_scores(test_text)["compound"]]])
X_combined = np.hstack((X_input, sentiment))
pred_label = model.predict(X_combined)[0]
emotion_labels = {0: "sad", 1: "joy", 2: "love", 3: "anger", 4: "fear", 5: "surprise"}
emotion_label = emotion_labels[pred_label]

print(f"\nüéØ Detected Emotion: {emotion_label.upper()}")

# GPT-2 Fine-Tune and Predict
mental_health_llm = MentalHealthLLM()
mental_health_llm.initialize_model()
mental_health_llm.train_model()  # Fine-tune
gpt2_response = mental_health_llm.predict(emotion_label, test_text)

# FLAN-T5 Response
light_llm = LightMentalHealthLLM()
flan_response = light_llm.get_advice(emotion_label, test_text)

# Unified Output
print("\nü§ñ GPT-2 Advice (Fine-Tuned):")
print("-" * 60)
print(gpt2_response)

print("\nüí° FLAN-T5 Suggestion:")
print("-" * 60)
print(flan_response)

print("\n‚úÖ End of Session")

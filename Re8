# Mental Health Assistant (Jupyter Notebook)
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

# 1. Initialize Model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

# 2. Input Variables
emotion = "anxious"  # Try: sad/angry/joyful
user_text = "I can't stop worrying about my job interview tomorrow"

# 3. Generate Tips
def generate_tips(emotion, user_text):
    prompt = f"""
    Generate 3 practical mental health tips for someone feeling {emotion}. 
    User context: "{user_text}". 
    Respond concisely in bullet points:
    1. 
    2. 
    3. 
    """
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(**inputs, max_new_tokens=150)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# 4. Output
print("Mental Health Tips:")
print(generate_tips(emotion, user_text))



# Mental-Health-Mistral (4-bit quantized for 8GB GPU)
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "GRMenon/mental-health-mistral-7b-instructv0.2-finetuned-V2", 
    quantization_config=bnb_config
)
